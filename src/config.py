# Hyperparameters

# What's the optimal vocabulary size?
vocabulary_size = 4000  # Number of tokens in vocabulary post-BPE
context_size = 8  # Number of tokens in a single training sequence
batch_size = 32  # Number of training examples per batch
embedding_dimension = 256 # Vector dimension for token embeddings
scale_factor = 0.01  # TODO: Add description comment for this hyperparameter