{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "195821c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "import math\n",
    "import numpy as np\n",
    "from config import seq_len, batch_size, d_model, vocab_size\n",
    "from utils import tokenize, clean, encode_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c761379",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean('../data/raw_corpus.txt', '../data/clean_corpus.txt')\n",
    "\n",
    "with open('../data/clean_corpus.txt', 'r', encoding='utf-8') as f:\n",
    "    corpus = f.read()\n",
    "\n",
    "tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c3f5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 128, 128)\n",
      "(32, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "tokenized_corpus = np.load('../data/tokenized_corpus_ids.npy')\n",
    "\n",
    "n_sequences = len(tokenized_corpus) - seq_len\n",
    "\n",
    "# Make training examples with stride = 1\n",
    "x_sequences = np.array([tokenized_corpus[i:i+seq_len] for i in range(n_sequences)])\n",
    "y_sequences = np.array([tokenized_corpus[i+1:i+seq_len+1] for i in range(n_sequences)])\n",
    "\n",
    "n_batches = len(x_sequences) // batch_size\n",
    "\n",
    "x_batches = np.array_split(x_sequences[:n_batches*batch_size], n_batches)\n",
    "y_batches = np.array_split(y_sequences[:n_batches*batch_size], n_batches)\n",
    "\n",
    "# Consider using Xavier initialization here\n",
    "embedding_factor = 1 / math.sqrt(d_model)  # Scale factor for embedding matrix\n",
    "embedding_matrix = np.random.randn(vocab_size, d_model) * embedding_factor\n",
    "\n",
    "# Example: 0th batch\n",
    "X = embedding_matrix[x_batches[0]]\n",
    "X = encode_position(X)\n",
    "\n",
    "# Attention\n",
    "## Initialize weights (Xavier normal)\n",
    "def xavier_normal(fan_in, fan_out, shape=None):\n",
    "    if shape is None:\n",
    "        shape = (fan_in, fan_out)\n",
    "\n",
    "    std = np.sqrt(2 / (fan_in + fan_out))\n",
    "\n",
    "    return np.random.normal(loc=0, scale=std, size=shape)\n",
    "\n",
    "W_K = xavier_normal(d_model, d_model)\n",
    "W_Q = xavier_normal(d_model, d_model)\n",
    "W_V = xavier_normal(d_model, d_model)\n",
    "\n",
    "## Compute K, Q, and V\n",
    "K = X @ W_K  # shapes: (batch_size, seq_len, d_model) @ (d_model, d_model) = (batch_size, seq_len, d_model)\n",
    "Q = X @ W_Q\n",
    "V = X @ W_V\n",
    "\n",
    "## TODO: Split\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
