{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "195821c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "import math\n",
    "import numpy as np\n",
    "from config import seq_len, batch_size, d_model, vocab_size, n_heads\n",
    "from utils import tokenize, clean, encode_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c761379",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean('../data/raw_corpus.txt', '../data/clean_corpus.txt')\n",
    "\n",
    "with open('../data/clean_corpus.txt', 'r', encoding='utf-8') as f:\n",
    "    corpus = f.read()\n",
    "\n",
    "tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3c3f5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = np.load('../data/tokenized_corpus_ids.npy')\n",
    "\n",
    "n_sequences = len(tokenized_corpus) - seq_len\n",
    "\n",
    "# Make training examples with stride = 1\n",
    "x_sequences = np.array([tokenized_corpus[i:i+seq_len] for i in range(n_sequences)])\n",
    "y_sequences = np.array([tokenized_corpus[i+1:i+seq_len+1] for i in range(n_sequences)])\n",
    "\n",
    "n_batches = len(x_sequences) // batch_size\n",
    "\n",
    "x_batches = np.array_split(x_sequences[:n_batches*batch_size], n_batches)\n",
    "y_batches = np.array_split(y_sequences[:n_batches*batch_size], n_batches)\n",
    "\n",
    "# Consider using Xavier initialization here\n",
    "embedding_factor = 1 / math.sqrt(d_model)  # Scale factor for embedding matrix\n",
    "embedding_matrix = np.random.randn(vocab_size, d_model) * embedding_factor\n",
    "\n",
    "# Example: 0th batch\n",
    "X = embedding_matrix[x_batches[0]]\n",
    "X = encode_position(X)\n",
    "\n",
    "# Attention\n",
    "## Initialize weights (Xavier normal)\n",
    "def xavier_normal(fan_in, fan_out, shape=None):\n",
    "    if shape is None:\n",
    "        shape = (fan_in, fan_out)\n",
    "\n",
    "    std = np.sqrt(2 / (fan_in + fan_out))\n",
    "\n",
    "    return np.random.normal(loc=0, scale=std, size=shape)\n",
    "\n",
    "W_K = xavier_normal(d_model, d_model)\n",
    "W_Q = xavier_normal(d_model, d_model)\n",
    "W_V = xavier_normal(d_model, d_model)\n",
    "\n",
    "## Compute K, Q, and V\n",
    "K = X @ W_K  # shapes: (batch_size, seq_len, d_model) @ (d_model, d_model) = (batch_size, seq_len, d_model)\n",
    "Q = X @ W_Q\n",
    "V = X @ W_V\n",
    "\n",
    "## TODO: Better way to reshape and transpose when axes are kept (?)\n",
    "\n",
    "## Split\n",
    "d_k = d_model // n_heads\n",
    "\n",
    "K = K.reshape(batch_size, seq_len, n_heads, d_k).transpose(0, 2, 1, 3)  # shapes: (batch_size, n_heads, seq_len, d_k)\n",
    "Q = Q.reshape(batch_size, seq_len, n_heads, d_k).transpose(0, 2, 1, 3)\n",
    "V = V.reshape(batch_size, seq_len, n_heads, d_k).transpose(0, 2, 1, 3)\n",
    "\n",
    "## Get scores\n",
    "scores = (Q @ K.transpose(0, 1, 3, 2)) / np.sqrt(d_k)  # shapes: (batch_size, n_heads, seq_len, d_k) @ (batch_size, n_heads, d_k, seq_len) = (batch_size, n_heads, seq_len, seq_len)\n",
    "\n",
    "## TODO: Masking\n",
    "## TODO: Softmax\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
