{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "195821c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "import math\n",
    "import numpy as np\n",
    "from config import seq_len, batch_size, d_model, vocab_size\n",
    "from utils import tokenize, clean, encode_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c761379",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean('../data/raw_corpus.txt', '../data/clean_corpus.txt')\n",
    "\n",
    "with open('../data/clean_corpus.txt', 'r', encoding='utf-8') as f:\n",
    "    corpus = f.read()\n",
    "\n",
    "tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c3f5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-8.80616786e-02 -4.39130152e-03  7.72558969e-03 ...  1.46430895e-01\n",
      "  -8.41336007e-03  5.85868499e-02]\n",
      " [-6.75439550e-02  6.41043421e-02 -9.61822785e-02 ... -4.23195761e-02\n",
      "   6.18879602e-02  3.15545277e-02]\n",
      " [ 1.98467025e-02 -8.78545684e-02 -6.16623535e-02 ... -8.88480949e-02\n",
      "  -1.32007605e-02 -4.07274055e-03]\n",
      " ...\n",
      " [-1.20466581e-02 -9.36947741e-02  8.05500370e-02 ...  2.17935640e-01\n",
      "  -3.45430508e-02 -1.12319345e-01]\n",
      " [-1.22404076e-01  2.14786922e-02  5.13239750e-02 ...  8.86542853e-05\n",
      "  -8.15786489e-02  1.30526526e-01]\n",
      " [ 6.10368223e-02 -9.32043901e-02  3.44309413e-03 ... -1.18576939e-01\n",
      "   9.18917164e-02  4.61330598e-02]] (128, 128)\n"
     ]
    }
   ],
   "source": [
    "tokenized_corpus = np.load('../data/tokenized_corpus_ids.npy')\n",
    "\n",
    "sequences = len(tokenized_corpus) - seq_len\n",
    "\n",
    "# Make training examples with stride = 1\n",
    "x = np.array([tokenized_corpus[i:i+seq_len] for i in range(sequences)])\n",
    "y = np.array([tokenized_corpus[i+1:i+seq_len+1] for i in range(sequences)])\n",
    "\n",
    "batches = len(x) // batch_size\n",
    "\n",
    "x_batches = np.array_split(x[:batches*batch_size], batches)\n",
    "y_batches = np.array_split(y[:batches*batch_size], batches)\n",
    "\n",
    "# Consider using Xavier initialization here\n",
    "embedding_factor = 1 / math.sqrt(d_model)  # Scale factor for embedding matrix\n",
    "embedding_matrix = np.random.randn(vocab_size, d_model) * embedding_factor\n",
    "\n",
    "# Example: 0th batch\n",
    "# embedded_batch = embedding_matrix[x_batches[0]]\n",
    "# encode_position(embedded_batch)\n",
    "\n",
    "# Attention\n",
    "## Initialize weights (Xavier normal)\n",
    "def xavier_normal(fan_in, fan_out, shape=None):\n",
    "    if shape is None:\n",
    "        shape = (fan_in, fan_out)\n",
    "\n",
    "    std = np.sqrt(2 / (fan_in + fan_out))\n",
    "\n",
    "    return np.random.normal(loc=0, scale=std, size=shape)\n",
    "\n",
    "W_Q = xavier_normal(d_model, d_model)\n",
    "W_K = xavier_normal(d_model, d_model)\n",
    "W_V = xavier_normal(d_model, d_model)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
